# CODEX

This file tracks project structure and changes. Update it whenever new files,
features, or behaviors are added.

## Purpose
- Provide a quick map of the project layout.
- Record recent changes for future requests.

## Structure
- `Makefile`: Unified entrypoints for checks, preset listing, local benchmarks, AutoJudge runs, and Docker CPU/GPU flows.
- `benchmarks/bench_speculative.py`: Benchmark runner for baseline, speculative, AutoJudge, and SpecExec methods. Supports HF models, KV cache, quantization, resume mode, enriched JSONL logging, and per-run error persistence.
- `configs/models.json`: Model presets (target/draft HF models, device, dtype, quantization, tokenizer, chat template).
- `configs/methods.json`: Method presets (baseline/speculative/autojudge/specexec/all).
- `configs/experiments.json`: Target/draft pairing presets for common comparisons.
- `configs/method_templates.json`: Templates for AutoJudge and SpecExec configs with parameters/metrics.
- `scripts/validate_configs.py`: Static validator for configs (cross-file references, method constraints, tokenizer compatibility for target/draft pair methods).
- `scripts/validate_results_jsonl.py`: JSONL schema/type validator for benchmark outputs (strict mode supported).
- `scripts/install_dependencies.sh`: Safe host bootstrap script (idempotent apt/pip install, optional GPU extras, EOL Ubuntu guard, never touches NVIDIA driver packages).
- `.github/workflows/ci.yml`: GitHub Actions CI pipeline (checks, tests, toy SpecExec smoke run, result-schema validation).
- `.python-version`: Recommended local Python runtime version.
- `datasets/.gitkeep`: Repository placeholder for local dataset directory (`datasets/`).
- `README.MD`: Project overview, usage, and examples.
- `sp_samp/autojudge.py`: AutoJudge implementation (synthetic label collection, judge training, and decoding).
- `sp_samp/specexec.py`: CPU SpecExec implementation (exact target sampling with draft-tree cache prefill and pruning).
- `sp_samp/hf_specexec.py`: HF SpecExec implementation with KV-cache reuse along prefix-tree edges.
- `sp_samp/cli.py`: Unified CLI runner (`bench`, `autojudge`, `specexec`, preset application).
- `sp_samp/__init__.py`: Public exports with optional lazy behavior for torch-dependent modules.
- `sp_samp/hf_adapter.py`: HF model adapter with KV cache and optional bitsandbytes quantization.
- `sp_samp/hf_sampling.py`: HF sampling (baseline + speculative) using KV cache.
- `sp_samp/sampling.py`: Reference CPU implementations + metrics.
- `sp_samp/mtbench.py`: MT-Bench loader.
- `sp_samp/methods/`: Method-facing exports (including SpecExec export).
- `tests/test_sampling.py`: Core correctness tests for baseline/speculative sampling.
- `tests/test_autojudge.py`: Unit tests for AutoJudge features, classifier fitting, and stats.
- `tests/test_specexec.py`: Unit tests for SpecExec behavior and stats.
- `Dockerfile`, `Dockerfile.gpu`: CPU/GPU containers.
- `requirements.txt`, `requirements-gpu.txt`: Dependencies.

## Recent Changes (2026-02-13)
- Standardized local dataset layout to `datasets/`:
- added tracked directory placeholder `datasets/.gitkeep`;
- Makefile defaults now use `DATASET=datasets/mt_bench.jsonl` and `OUT=datasets/results.jsonl`;
- benchmark runner default output path now resolves to `datasets/results.jsonl`;
- benchmark targets now fail fast if dataset file does not exist (clear recovery message);
- Docker dataset mount now resolves from absolute host path derived from `DATASET`;
- README examples updated to use `datasets/` paths.
- Updated ignore rules for local data:
- `.gitignore` now ignores `datasets/*` while keeping `datasets/.gitkeep`;
- `.dockerignore` now excludes `datasets/` from image build context.
- Added CI pipeline (`.github/workflows/ci.yml`) on Python 3.11:
- runs `make check`, `make test`, toy SpecExec smoke benchmark, and JSONL schema validation.
- Pinned dependency versions in `requirements.txt` and `requirements-gpu.txt` for reproducible installs.
- Added `.python-version` with recommended runtime `3.11`.
- Improved bootstrap interpreter selection:
- `scripts/install_dependencies.sh` now prefers `python3.11` when available, enforces minimum Python 3.10, and warns when below 3.11.
- Added benchmark result schema validation:
- new script `scripts/validate_results_jsonl.py`;
- new `make validate-results` target with strict mode.
- Removed `autojudge_placeholder` from `configs/methods.json` and simplified `scripts/validate_configs.py` logic accordingly.
- Fixed Docker/Torch version consistency for GPU builds:
- `Makefile` now exposes `TORCH_INDEX_URL` and `TORCH_VERSION` variables;
- default `TORCH_VERSION` aligned to `2.5.1` (same baseline as `requirements.txt`);
- README GPU build example updated accordingly.
- Improved local tooling reliability:
- `Makefile` now prefers `.venv/bin/python` when present and falls back to `python3`.
- `make test` now validates `pytest` availability first and emits a clear recovery message (`make setup`).
- Fixed SpecExec summary metric correctness:
- in benchmark stat aggregation, `max_active_branches` is now aggregated as `max(...)` instead of sum.
- Replaced stale AutoJudge placeholder module export:
- `sp_samp/methods/autojudge/__init__.py` now exports actual AutoJudge APIs when dependencies are installed;
- when torch/transformers are missing, module now raises a clear dependency error on attribute access.
- Expanded `.dockerignore` to reduce Docker context size and avoid shipping local artifacts (`.venv`, caches, build outputs, logs, `papers/`).

## Recent Changes (2026-02-10)
- Added safe host bootstrap flow:
- new script `scripts/install_dependencies.sh` installs missing Ubuntu packages and updates Python deps in `.venv`;
- supports optional GPU extras via `--gpu`;
- blocks on EOL Ubuntu by default (can be overridden with `--allow-eol-ubuntu`);
- explicitly avoids NVIDIA driver/CUDA driver package changes.
- Added Make targets for bootstrap:
- `make setup`, `make setup-gpu`;
- `ALLOW_EOL_UBUNTU=1` passthrough support.
- Updated dependency baselines in `requirements.txt` and `requirements-gpu.txt` to newer minimum versions.
- Updated `README.MD` with safe installation workflow and Ubuntu EOL caveats.
- Added benchmark reliability/runtime instrumentation in `benchmarks/bench_speculative.py`:
- system metadata is written to each JSONL record (`git_sha`, host, GPU/driver, torch/transformers versions);
- resume mode skips already completed runs via `resume_key`;
- per-run exceptions are persisted to JSONL (`status=error`, traceback) without aborting the whole method.
- Added headless execution guard:
- new CLI/benchmark flag `--require-headless`;
- `Makefile` support via `HEADLESS=1`.
- Optimized HF SpecExec cache building in `sp_samp/hf_specexec.py`:
- draft tree expansion now reuses KV cache via `prefill + step`;
- target cache fill now uses depth-wise tree traversal with KV reuse.
- Reworked SpecExec to paper-aligned exact behavior:
- SpecExec now samples exactly from target distribution while using draft-tree cache prefill (no branch-selection sampling bias).
- Added stronger SpecExec diagnostics (`SpecExecError`) with stage/prefix context for faster failure triage.
- Added SpecExec distribution and exactness tests in `tests/test_specexec.py`:
- empirical distribution match to target;
- sequence equivalence to baseline for equal seeds.
- Implemented SpecExec as a first-class method:
- added `sp_samp/specexec.py` (toy/CPU) and `sp_samp/hf_specexec.py` (HF);
- added `SpecExecStats` with branch metrics (`branch_prune_rate`, `effective_parallelism`, `max_active_branches`, call rates).
- Integrated SpecExec into benchmark flow (`benchmarks/bench_speculative.py`):
- new method option `specexec`;
- `all` now runs baseline + speculative + autojudge + specexec;
- new args `--parallel-branches` and `--branch-prune-threshold`;
- JSONL records now include SpecExec configuration/metrics fields.
- Integrated SpecExec into CLI (`sp_samp/cli.py`):
- new subcommand `specexec`;
- method presets now accept `specexec`;
- passthrough for `parallel_branches` and `branch_prune_threshold`.
- Updated configs:
- `configs/methods.json`: added `specexec_k4`, updated `compare_all_k4`, removed SpecExec placeholder preset;
- `configs/experiments.json`: added SpecExec experiment presets for Llama3/Mistral/GPT-OSS;
- `configs/method_templates.json`: aligned SpecExec template metrics with actual output.
- Updated automation entrypoints:
- `Makefile`: added `SPECEXEC_EXPERIMENT`, `specexec`, and `docker-specexec` targets; refreshed `bench-all` description.
- Added unit tests for SpecExec in `tests/test_specexec.py`.
- Updated `README.MD` with SpecExec usage examples and metrics documentation.
- Added `scripts/validate_configs.py` and integrated it into `make check` and `make validate-configs`.
- Added `make smoke-hf` for quick end-to-end HF pipeline verification with a tiny model.
- Added `TARGET_PRESET`/`DRAFT_PRESET` variables in `Makefile` for `bench-method` to avoid hardcoded mismatched pairs.
- Fixed benchmark import bug by explicitly importing `JudgeMLP` in `benchmarks/bench_speculative.py`.
- Fixed AutoJudge run determinism path by forwarding per-run `seed` into `autojudge_sample_hf`.
- Improved HF architecture in `benchmarks/bench_speculative.py`:
- separated target/draft runtime settings (`draft_tokenizer`, `draft_device`, `draft_dtype`, `draft_quant`, `draft_bnb_compute_dtype`);
- stopped forcing draft to use target tokenizer path;
- added tokenizer compatibility guard for speculative/AutoJudge methods;
- avoided unnecessary draft-model construction for baseline-only runs.
- Expanded benchmark JSONL records with resolved target/draft runtime fields for reproducibility.
- Expanded CLI run args in `sp_samp/cli.py` to pass draft-specific overrides.
- Added draft-preset mapping in `sp_samp/cli.py` to avoid target-arg overwrites.
- Updated `configs/experiments.json` to correctness-safe target/draft pairings (identical tokenizer presets).
- Updated `Makefile` defaults to valid experiment IDs (`llama3_all_methods`, `llama3_target_llama3_autojudge_k4`).
- Updated `README.MD` with config validation and HF smoke targets, and refreshed experiment examples.
- Updated `README.MD` HF command examples to tokenizer-compatible target/draft pairs.
- Added preset configs for models and methods in `configs/`.
- Added experiment pair presets in `configs/experiments.json`.
- Added AutoJudge/SpecExec config templates in `configs/method_templates.json`.
- Added CLI runner in `sp_samp/cli.py`.
- Added CLI support for `--experiment` presets.
- Exposed benchmark parser/run for programmatic use.
- Added `benchmarks/__init__.py` for importable benchmark module.
- Added CODEX.MD and README.MD.
- Expanded README with a full from-zero setup guide.
- Fixed GPU requirements to avoid reinstalling CPU-only torch.
- Implemented `sp_samp/autojudge.py` with:
- synthetic judge-label collection from target/draft models;
- MLP judge classifier training;
- AutoJudge decoding loop with threshold decisions and fallback-to-target.
- Integrated AutoJudge into `benchmarks/bench_speculative.py`:
- new methods: `autojudge`, `all`;
- AutoJudge train/inference args;
- method-specific JSONL metrics.
- Updated CLI for method selection:
- `bench --method autojudge|all`;
- `autojudge` command now routes to benchmark runner.
- Updated configs:
- new method presets `autojudge_k4`, `compare_all_k4`;
- new experiment presets for AutoJudge and all-method comparisons.
- Added tests in `tests/test_autojudge.py`.
- Updated `README.MD` with end-to-end usage for AutoJudge and all-method runs.
- Added `Makefile` with:
- `make help`, `make check`, `make list-presets`;
- quick local run (`make bench-toy`);
- preset runs (`make bench`, `make autojudge`, `make bench-all`);
- Docker runs (`make docker-build`, `make docker-build-gpu`, `make docker-bench`, `make docker-autojudge`, `make docker-bench-all`).
- Simplified `sp_samp/cli.py`:
- removed duplicated parser arguments via shared helper;
- switched to lazy benchmark import so `list-presets` works without ML dependencies.
- expanded passthrough arguments for toy/HF parity (`vocab_size`, `draft_noise`, `seed`).
- Hardened package import behavior in `sp_samp/__init__.py`:
- moved HF/AutoJudge exports to optional imports, preserving lightweight commands without `torch`.
- Improved `benchmarks/bench_speculative.py` portability:
- supports toy-mode execution without `torch`;
- emits clear dependency errors only for HF/AutoJudge paths.
- changed toy default `vocab_size` to `2048` (from `32000`) to avoid pathological memory/time in `RandomModel`.
- added fail-fast validation for `autojudge` without HF model arguments.
- Standardized benchmark invocations to module mode (`python -m benchmarks.bench_speculative`) in docs and Makefile.
- Tuned `make bench-toy` defaults for fast smoke runs.
